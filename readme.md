# llmcpp

## 概要
llmcpp は、oobabooga/text-generation-webui(https://github.com/oobabooga/text-generation-webui) により提供される Open-API 互換の Web API を利用して、ローカル環境でテキスト生成する作業を支援する CLI のフロントエンドである。

## 事前準備
1. `text-generation-webui/user_data/CMD_FLAGS.txt` にて ` --api` オプションを指定する。
2. `start_windows.bat` など実行環境と対応するスクリプトを実行し、サーバーを起動する。

## チュートリアル
llmcpp はデフォルトで下記のファイルを読み込む。

| ファイル名 | 概要 | 変更するオプション |
| --- | --- | --- |
| system_prompts.txt | システムプロンプト | --system-prompts-file |
| examples.txt | 生成したいテキストの例 | --examples-file |
| history.txt | 直近に生成されたテキスト | --history-file |

ログはデフォルトで下記に出力される。

* log.txt

ログをコンソールに冗長に出力する場合、`-v` オプションを指定する。

```
> llmcpp -v
```

ログに出力する情報を範囲を変更する場合、 `--log-level` オプションに続き、`(trace|debug|info|warning|error|fatal)` のいずれか指定する。デフォルトでは `info` が使用される。

必要に応じて下記のオプションで通信先を指定する。oobabooga をデフォルトの設定で運用している場合、明示的に指定する必要はない。

* `--host`
* `--port`
* `--api-key`

実行を開始すると、 llmcpp は下記のように LLM に渡すプロンプトを作成する。

1. `system_prompts.txt` の内容をプロンプトに追加する。
2. `history.txt` の内容を、末尾の行から優先して可能な限り、本来の順序でプロンプトに追加する。
3. `examples.txt` の内容を、可能な限り、本来の順序でプロンプトに追加する。
4. `--generation-prefix` オプションで指定された文字列をプロンプトに追加する。

LLM に渡すプロンプトは、
ここでいう可能な限りとは、「`--max-total-context-tokens` オプションで指定しているコンテキストの最大トークン数(デフォルトの値は `4096`)から、`--max-tokens` オプションで指定している LLM により生成されるテキストの最大トークン数(デフォルトの値は `512`)を差し引いたトークン数を超過しない限り」を意味する。
つまり llmcpp は、「プロンプト」と「LLM により生成されるテキスト」を足した全体のトークン数が、コンテキストの最大トークン数以下に収まるよう、`history.txt` や `examples.txt` から読み込むテキストの量を調整する。

プロンプトを作成した後、LLM と通信し、テキストを生成する。
生成されたテキストの先頭には `--generation-prefix` オプションで指定された文字列が含まれる。
デフォルトでは、生成したテキストは `history.txt` の末尾に追加される。変更する場合は `--output-file` で出力先を指定する。
したがって、繰り返しテキストを生成すると、`history.txt` の内容は増加し、いつかその全量がコンテキストの最大トークン数に収まらなくなる。
前述の戦略により、`history.txt` の内容は増加した後においても、可能な限り最新の生成されたテキストをプロンプトに含める。

LLMに小説を生成させる場合、`history.txt` は作成された文章になる。
LLMとチャットをする場合、`history.txt` は会話の履歴になる。

## 反復
`-N` オプションにより処理を反復する回数を指定することができる。デフォルトの値は `1` である。`-1` を指定すると実行を停止するまで無限に処理を反復する。

## {{phase}} マクロ
1回の処理は1個以上の phase から構成され、それらは順番に実行される。
phase は `--phases "MyPhase1" "MyPhase2" "MyPhase3"` オプションで任意の数だけ指定可能である。 
現在実行中の phase は、プロンプトの内容に `{{phase}}` マクロを記述することにより、実行時に参照できる。
`--mode chat` オプションを指定することは、`--phases "{{char}}" "{{user}}" --generation-prefix "{{phase}}}: "`オプションを指定することと等価である。
すなわち、 `chat` モードでは、pahse として現在発言している人物の名前を扱い、それをプロンプトの先頭に追加することにより、LLM が当該の人物の発言を生成するように誘導する。